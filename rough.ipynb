{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"spider_data/train_spider.json\", 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(\"spider_data/dev.json\", 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "with open(\"spider_data/tables.json\", 'r') as f:\n",
    "    tables = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 1034, 166)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dev_data), len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department_management\n",
      "SELECT count(*) FROM head WHERE age  >  56\n",
      "['SELECT', 'count', '(', '*', ')', 'FROM', 'head', 'WHERE', 'age', '>', '56']\n",
      "['select', 'count', '(', '*', ')', 'from', 'head', 'where', 'age', '>', 'value']\n",
      "How many heads of the departments are older than 56 ?\n",
      "['How', 'many', 'heads', 'of', 'the', 'departments', 'are', 'older', 'than', '56', '?']\n",
      "{'from': {'table_units': [['table_unit', 1]], 'conds': []}, 'select': [False, [[3, [0, [0, 0, False], None]]]], 'where': [[False, 3, [0, [0, 10, False], None], 56.0, None]], 'groupBy': [], 'having': [], 'orderBy': [], 'limit': None, 'intersect': None, 'union': None, 'except': None}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0]['db_id'])\n",
    "print(train_data[0]['query'])\n",
    "print(train_data[0]['query_toks'])\n",
    "print(train_data[0]['query_toks_no_value'])\n",
    "print(train_data[0]['question'])\n",
    "print(train_data[0]['question_toks'])\n",
    "print(train_data[0]['sql'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['column_names', 'column_names_original', 'column_types', 'db_id', 'foreign_keys', 'primary_keys', 'table_names', 'table_names_original'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['body_builder', 'people']\n",
      "[[-1, '*'], [0, 'Body_Builder_ID'], [0, 'People_ID'], [0, 'Snatch'], [0, 'Clean_Jerk'], [0, 'Total'], [1, 'People_ID'], [1, 'Name'], [1, 'Height'], [1, 'Weight'], [1, 'Birth_Date'], [1, 'Birth_Place']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tables[4]['table_names_original']), print(tables[4]['column_names_original'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['department', 'head', 'management']\n",
      "[[-1, '*'], [0, 'Department_ID'], [0, 'Name'], [0, 'Creation'], [0, 'Ranking'], [0, 'Budget_in_Billions'], [0, 'Num_Employees'], [1, 'head_ID'], [1, 'name'], [1, 'born_state'], [1, 'age'], [2, 'department_ID'], [2, 'head_ID'], [2, 'temporary_acting']]\n"
     ]
    }
   ],
   "source": [
    "db_id = train_data[0]['db_id']\n",
    "for i in tables:\n",
    "    if i['db_id'] == db_id:\n",
    "        print(i['table_names_original'])\n",
    "        print(i['column_names_original'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department_management\n",
      "department_management\n",
      "dict_keys(['column_names', 'column_names_original', 'column_types', 'db_id', 'foreign_keys', 'primary_keys', 'table_names', 'table_names_original'])\n",
      "[[-1, '*'], [0, 'department id'], [0, 'name'], [0, 'creation'], [0, 'ranking'], [0, 'budget in billions'], [0, 'num employees'], [1, 'head id'], [1, 'name'], [1, 'born state'], [1, 'age'], [2, 'department id'], [2, 'head id'], [2, 'temporary acting']]\n",
      "[[-1, '*'], [0, 'Department_ID'], [0, 'Name'], [0, 'Creation'], [0, 'Ranking'], [0, 'Budget_in_Billions'], [0, 'Num_Employees'], [1, 'head_ID'], [1, 'name'], [1, 'born_state'], [1, 'age'], [2, 'department_ID'], [2, 'head_ID'], [2, 'temporary_acting']]\n"
     ]
    }
   ],
   "source": [
    "data_dbid = train_data[0]['db_id']\n",
    "for i in range(len(tables)):\n",
    "    if tables[i]['db_id'] == data_dbid:\n",
    "        table_dbid = tables[i]['db_id']\n",
    "        print(data_dbid)\n",
    "        print(table_dbid)\n",
    "        print(tables[i].keys())\n",
    "        print(tables[i]['column_names'])\n",
    "        print(tables[i]['column_names_original'])\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SpiderDataset...\n",
      "Loaded 7000 training examples.\n",
      "\n",
      "--- Dataset Sample Examples ---\n",
      "Sample 1:\n",
      "  Query: How many heads of the departments are older than 56 ?\n",
      "  Schema: | department ; *, department id, name, creation, ranking, budget in billions, num employees | head ; *, head id, name, born state, age | management ; *, department id, head id, temporary acting |\n",
      "  SQL Query: SELECT count(*) FROM head WHERE age  >  56\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SpiderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for the Spider Text-to-SQL task, combining the query and the\n",
    "    database schema into a structured input string.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_data_path=None, table_data_path=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading and processing the train and schema files.\n",
    "        \"\"\"\n",
    "        print(\"Initializing SpiderDataset...\")\n",
    "\n",
    "        # --- 1. Load Data ---\n",
    "        try:\n",
    "            if train_data_path:\n",
    "                with open(train_data_path, 'r') as f:\n",
    "                    self.data = json.load(f)\n",
    "            else:\n",
    "                self.data = json.load(io.StringIO(MOCK_TRAIN_JSON_CONTENT))\n",
    "\n",
    "            if table_data_path:\n",
    "                with open(table_data_path, 'r') as f:\n",
    "                    table_list = json.load(f)\n",
    "            else:\n",
    "                table_list = json.load(io.StringIO(MOCK_TABLES_JSON_CONTENT))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading JSON files. Check file paths/content: {e}\")\n",
    "            self.data = []\n",
    "            table_list = []\n",
    "\n",
    "        # --- 2. Process Schemas for Quick Lookup ---\n",
    "        self.schema_map = {}\n",
    "        for db_schema in table_list:\n",
    "            db_id = db_schema['db_id']\n",
    "            formatted_schema = self._format_schema(db_schema)\n",
    "            self.schema_map[db_id] = formatted_schema\n",
    "\n",
    "        print(f\"Loaded {len(self.data)} training examples.\")\n",
    "\n",
    "\n",
    "    def _format_schema(self, db_schema):\n",
    "        \"\"\"\n",
    "        Converts the database schema dictionary into the required string format:\n",
    "        \"| table_1 ; *, col_1, col_2, ... | table_2 ; *, col_1, col_2, ... |\"\n",
    "        \"\"\"\n",
    "        table_names = db_schema['table_names']\n",
    "        column_names_info = db_schema['column_names']\n",
    "\n",
    "        table_to_columns = {name: [] for name in table_names}\n",
    "        \n",
    "        for col_info in column_names_info:\n",
    "            table_idx = col_info[0]\n",
    "            col_name = col_info[1]\n",
    "            \n",
    "            # Skip the global '*' column (table_idx = -1)\n",
    "            if table_idx >= 0:\n",
    "                 table_name = table_names[table_idx]\n",
    "                 table_to_columns[table_name].append(col_name)\n",
    "\n",
    "        schema_parts = []\n",
    "        for table_name in table_names:\n",
    "            columns = table_to_columns[table_name]\n",
    "            \n",
    "            # Prepend the '*' column for universal selection\n",
    "            full_columns = ['*'] + columns\n",
    "            columns_str = \", \".join(full_columns)\n",
    "            \n",
    "            schema_parts.append(f\"| {table_name} ; {columns_str} \")\n",
    "\n",
    "        return \"\".join(schema_parts) + \"|\"\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the training set.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves one sample from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (query, formatted_schema, gold_sql)\n",
    "        \"\"\"\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        query = sample['question']\n",
    "        gold_sql = sample['query'] \n",
    "        \n",
    "        db_id = sample['db_id']\n",
    "        formatted_schema = self.schema_map.get(db_id, \"Error: Schema not found\")\n",
    "    \n",
    "        return query, formatted_schema, gold_sql\n",
    "\n",
    "spider_data = SpiderDataset(train_data_path=\"spider_data/train_spider.json\", table_data_path=\"spider_data/tables.json\")\n",
    "    \n",
    "print(\"\\n--- Dataset Sample Examples ---\")\n",
    "\n",
    "# 2. Access samples and confirm the output format\n",
    "for i in range(len(spider_data)):\n",
    "    query, schema, sql = spider_data[i]\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Query: {query}\")\n",
    "    print(f\"  Schema: {schema}\")\n",
    "    print(f\"  SQL Query: {sql}\") \n",
    "    print(\"-\" * 40)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, sch, ans = spider_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| department ; *, department id, name, creation, ranking, budget in billions, num employees | head ; *, head id, name, born state, age | management ; *, department id, head id, temporary acting |'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many acting statuses are there?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/projects/p32722/Models/Qwen2.5-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderDataset:\n",
    "    \"\"\"\n",
    "    Dataset class for inference on Spider dataset.\n",
    "    Returns samples in a format suitable for model prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, table_data_path):\n",
    "        \"\"\"Load the evaluation dataset and schema information.\"\"\"\n",
    "        print(f\"Loading dataset from {data_path}...\")\n",
    "        \n",
    "        with open(data_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        with open(table_data_path, 'r') as f:\n",
    "            table_list = json.load(f)\n",
    "        \n",
    "        # Build schema map\n",
    "        self.schema_map = {}\n",
    "        for db_schema in table_list:\n",
    "            db_id = db_schema['db_id']\n",
    "            formatted_schema = self._format_schema(db_schema)\n",
    "            self.schema_map[db_id] = formatted_schema\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} examples for inference.\")\n",
    "    \n",
    "    def _format_schema(self, db_schema):\n",
    "        \"\"\"Format database schema into string representation.\"\"\"\n",
    "        table_names = db_schema['table_names']\n",
    "        column_names_info = db_schema['column_names']\n",
    "        \n",
    "        table_to_columns = {name: [] for name in table_names}\n",
    "        for col_info in column_names_info:\n",
    "            table_idx = col_info[0]\n",
    "            col_name = col_info[1]\n",
    "            \n",
    "            if table_idx >= 0:\n",
    "                table_name = table_names[table_idx]\n",
    "                table_to_columns[table_name].append(col_name)\n",
    "        \n",
    "        schema_parts = []\n",
    "        for table_name in table_names:\n",
    "            columns = table_to_columns[table_name]\n",
    "            full_columns = ['*'] + columns\n",
    "            columns_str = \", \".join(full_columns)\n",
    "            schema_parts.append(f\"| {table_name} ; {columns_str} \")\n",
    "        \n",
    "        return \"\".join(schema_parts) + \"|\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return query, schema, db_id for inference.\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        query = sample['question']\n",
    "        db_id = sample['db_id']\n",
    "        formatted_schema = self.schema_map.get(db_id, \"\")\n",
    "\n",
    "        reference, template = get_sql_template_and_reference(formatted_schema, gold_sql)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'schema': formatted_schema,\n",
    "            'db_id': db_id,\n",
    "            'gold_sql': sample.get('query', ''),  # For gold file generation\n",
    "            \"reference\": reference,\n",
    "            \"template\": template\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from spider_data/dev.json...\n",
      "Loaded 1034 examples for inference.\n"
     ]
    }
   ],
   "source": [
    "spider = SpiderDataset(data_path=\"spider_data/dev.json\", table_data_path=\"spider_data/tables.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Show name, country, age for all singers ordered by age from the oldest to the youngest.',\n",
       " 'schema': '| stadium ; *, stadium id, location, name, capacity, highest, lowest, average | singer ; *, singer id, name, country, song name, song release year, age, is male | concert ; *, concert id, concert name, theme, stadium id, year | singer in concert ; *, concert id, singer id |',\n",
       " 'db_id': 'concert_singer',\n",
       " 'gold_sql': 'SELECT name ,  country ,  age FROM singer ORDER BY age DESC',\n",
       " 'reference': '| singer; name, country, age |',\n",
       " 'template': 'SELECT _ ,  _ ,  _ FROM _ ORDER BY _ DESC'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spider[2]\n",
    "q = data['query']\n",
    "sch = data['schema']\n",
    "gold_sql = data['gold_sql']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "\n",
    "def parse_schema(schema_str: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"Parses a schema string into a dictionary mapping table names to column lists.\"\"\"\n",
    "    schema = {}\n",
    "    parts = [p.strip() for p in schema_str.split('|') if p.strip()]\n",
    "    for part in parts:\n",
    "        if ';' not in part:\n",
    "            continue\n",
    "        table_name_str, columns_str = [s.strip() for s in part.split(';', 1)]\n",
    "        columns = [c.strip() for c in columns_str.split(',')]\n",
    "        schema[table_name_str.lower()] = columns\n",
    "    return schema\n",
    "\n",
    "\n",
    "def normalize(name: str) -> str:\n",
    "    \"\"\"Normalize names: lowercase and remove underscores/spaces.\"\"\"\n",
    "    return re.sub(r'[\\s_]+', '', name.strip().lower())\n",
    "\n",
    "\n",
    "def extract_aliases(sql_query: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract aliases (alias -> real table).\"\"\"\n",
    "    alias_pattern = re.compile(\n",
    "        r'\\bFROM\\s+([a-zA-Z_][\\w]*)\\s+(?:AS\\s+)?([a-zA-Z_][\\w]*)'\n",
    "        r'|\\bJOIN\\s+([a-zA-Z_][\\w]*)\\s+(?:AS\\s+)?([a-zA-Z_][\\w]*)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    aliases = {}\n",
    "    for match in alias_pattern.finditer(sql_query):\n",
    "        t1, a1, t2, a2 = match.groups()\n",
    "        if t1 and a1:\n",
    "            aliases[a1.lower()] = t1.lower()\n",
    "        if t2 and a2:\n",
    "            aliases[a2.lower()] = t2.lower()\n",
    "    return aliases\n",
    "\n",
    "\n",
    "def get_sql_template_and_reference(schema_str: str, sql_query: str) -> Tuple[str, str]:\n",
    "    \"\"\"Generates reference and template for given SQL and schema.\"\"\"\n",
    "    schema = parse_schema(schema_str)\n",
    "    aliases = extract_aliases(sql_query)\n",
    "\n",
    "    # Flatten schema info\n",
    "    all_tables = list(schema.keys())\n",
    "    all_columns = {c for cols in schema.values() for c in cols}\n",
    "\n",
    "    # Normalization maps\n",
    "    norm_table_map = {normalize(t): t for t in all_tables}\n",
    "    norm_col_map = {normalize(c): c for c in all_columns}\n",
    "\n",
    "    # Extract potential tokens (tables, columns, aliases)\n",
    "    tokens = re.findall(r'[A-Za-z_][A-Za-z0-9_]*|\"[^\"]+\"|\\*', sql_query)\n",
    "\n",
    "    found_tables = set()\n",
    "    found_columns = set()\n",
    "\n",
    "    for token in tokens:\n",
    "        tok = token.strip('\"').lower()\n",
    "        tok_norm = normalize(tok)\n",
    "        # Match tables\n",
    "        if tok_norm in norm_table_map:\n",
    "            found_tables.add(norm_table_map[tok_norm])\n",
    "        # Match columns\n",
    "        if tok_norm in norm_col_map:\n",
    "            found_columns.add(norm_col_map[tok_norm])\n",
    "\n",
    "    # Include aliases’ base tables if they appear\n",
    "    for alias, table in aliases.items():\n",
    "        if table in schema:\n",
    "            found_tables.add(table)\n",
    "\n",
    "    # --- Build Reference ---\n",
    "    reference_parts = []\n",
    "    seen_pairs = set()\n",
    "\n",
    "    for table in found_tables:\n",
    "        table_cols = schema.get(table, [])\n",
    "        cols_for_table = [\n",
    "            col for col in table_cols\n",
    "            if normalize(col) in {normalize(fc) for fc in found_columns}\n",
    "        ]\n",
    "        if (table, tuple(cols_for_table)) not in seen_pairs:\n",
    "            reference_parts.append(\n",
    "                f\"{table}; {', '.join(cols_for_table)}\" if cols_for_table else f\"{table};\"\n",
    "            )\n",
    "            seen_pairs.add((table, tuple(cols_for_table)))\n",
    "\n",
    "    reference = f\"| {' | '.join(reference_parts)} |\" if reference_parts else \"| |\"\n",
    "\n",
    "    # --- Build Template ---\n",
    "    template = sql_query\n",
    "\n",
    "    # Replace alias.column → alias._\n",
    "    for alias, table in aliases.items():\n",
    "        if table in schema:\n",
    "            for col in schema[table]:\n",
    "                for variant in [col, col.replace(' ', '_')]:\n",
    "                    pattern = rf'\\b{alias}\\.{re.escape(variant)}\\b'\n",
    "                    template = re.sub(pattern, f'{alias}._', template, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace table.column → table._\n",
    "    for table in found_tables:\n",
    "        for col in schema.get(table, []):\n",
    "            for variant in [col, col.replace(' ', '_')]:\n",
    "                pattern = rf'\\b{table}\\.{re.escape(variant)}\\b'\n",
    "                template = re.sub(pattern, f'{table}._', template, flags=re.IGNORECASE)\n",
    "\n",
    "    # Replace remaining names with '_'\n",
    "    all_names = sorted(found_tables.union(found_columns), key=len, reverse=True)\n",
    "    for name in all_names:\n",
    "        for variant in [name, name.replace(' ', '_')]:\n",
    "            template = re.sub(rf'\\b{re.escape(variant)}\\b', '_', template, flags=re.IGNORECASE)\n",
    "\n",
    "    return reference, template\n",
    "\n",
    "for i in range(len(spider)-700):\n",
    "    # if i < 10:\n",
    "    sample =spider[i]\n",
    "    schema = sample['schema']\n",
    "    sql_query = sample['gold_sql']\n",
    "    query = sample['query']\n",
    "    reference, template = get_sql_template_and_reference(schema, sql_query)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"\\nSchema: \\\"{schema}\\\"\")\n",
    "    print(f\"SQL Query: \\\"{sql_query}\\\"\")\n",
    "    print(f\"NL Query: \\\"{query}\\\"\")\n",
    "    print(\"## Output\")\n",
    "    print(f\"**Reference:** `{reference}`\")\n",
    "    print(f\"**Template:** `{template}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"format(''' SELECT COUNT(*) FROM ( SELECT DISTINCT singer_id FROM singer_in_concert ) ''', {'table': 'singer'})\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_sql_query(sql_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean SQL query by extracting only the SQL code from markdown blocks\n",
    "    or removing markdown markers, newlines, and extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        sql_string: Raw SQL string that may contain ```sql, ```, \\n, and other text\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned SQL query as a single line with normalized spacing\n",
    "    \"\"\"\n",
    "    # First, try to extract SQL from within code blocks\n",
    "    # Pattern to match content between ```sql and ``` or ``` and ```\n",
    "    sql_block_pattern = r'```\\s*(?:sql)?\\s*(.*?)```'\n",
    "    match = re.search(sql_block_pattern, sql_string, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        # Extract only the SQL content from the code block\n",
    "        cleaned = match.group(1)\n",
    "    else:\n",
    "        # No code block found, clean the entire string\n",
    "        cleaned = sql_string\n",
    "        # Remove any stray ``` markers\n",
    "        cleaned = re.sub(r'```\\s*sql\\s*', '', cleaned, flags=re.IGNORECASE)\n",
    "        cleaned = re.sub(r'```', '', cleaned)\n",
    "    \n",
    "    # Remove all newline characters (\\n, \\r\\n, \\r)\n",
    "    cleaned = cleaned.replace('\\n', ' ')\n",
    "    cleaned = cleaned.replace('\\r', ' ')\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "sql_input = \"\"\"format(''' SELECT COUNT(*) FROM ( SELECT DISTINCT singer_id FROM singer_in_concert ) ''', {'table': 'singer'})\n",
    "\"\"\"\n",
    "\n",
    "clean_sql_query(sql_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" format('''\\n    SELECT COUNT(*) FROM {table}\\n    ''', {'table': 'singer'}) in ```sql\\n    (generated query)\\n    ```\\n    \""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"\"\"Given the database schema and question, generate the SQL query. Enclose the SQL query with in ```sql\n",
    "(generated query)```. Write it in a single line\n",
    "\n",
    "    Schema: | stadium ; *, stadium id, location, name, capacity, highest, lowest, average | singer ; *, singer id, name, country, song name, song release year, age, is male | concert ; *, concert id, concert name, theme, stadium id, year | singer in concert ; *, concert id, singer id |\n",
    "\n",
    "    Question: How many singers do we have?\n",
    "\n",
    "    SQL Query: format('''\n",
    "    SELECT COUNT(*) FROM {table}\n",
    "    ''', {'table': 'singer'}) in ```sql\n",
    "    (generated query)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "a.split('SQL Query:')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
